# Sistema de Segmentaci√≥n y Mensajer√≠a Personalizada

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.2+-orange.svg)
![Azure-OpenAI](https://img.shields.io/badge/Azure%20OpenAI-API-lightgrey)

Soluci√≥n t√©cnica para segmentaci√≥n de miembros y generaci√≥n de mensajes personalizados basados en comportamiento de churn.

# üìå Descripci√≥n

Este proyecto implementa:
1. **Segmentaci√≥n de miembros** mediante an√°lisis PCA y clustering
2. **Generaci√≥n de mensajes personalizados** usando Azure OpenAI
3. Integraci√≥n con datos anonimizados del sector telecomunicaciones

## üõ†Ô∏è Estructura del Proyecto

# üìä EDA

Para darle soluci√≥n al proyecto, se verifica que las fuentes de los datos provienen de dos sitios diferentes. El primero consta de datos anonimizados de una empresa de telecomunicaciones en B√©lgica, y el segundo conjunto de datos proviene de archivos en formato jsonl, los cuales contienen informaci√≥n de interacci√≥n de usuarios para un *e-commerce*.

1. **Churn**
para elaborar el EDA se llevo a acbo en visual Studio Code en un archivo .ipynb, evidenciando que se encuentran 11896 registros y 180 columnas los cuales estan anonimizados por cuestion de seguridad, y como target se encuentra la columna (y) la cual muestra si hay churn o no para un usuario, se obtiene que la probabilidad de churn del conjunto de datos es del 3.43%, y se le hizo un tratamiento a los clientes es decir comunicaci√≥n con ellos al 75.74%.

A continuaci√≥n, se visualiza una grafico de barras con las correlaciones absolutas de las variables con la columna objetivo o traget

![Correlaci√≥n de variables](https://github.com/NestorSaenz/prueba_tecnica_VF/blob/main/imagenes/correlacion.png)

Matriz de correlaci√≥n de variables

![Matriz de Correlaci√≥n de variables](https://github.com/NestorSaenz/prueba_tecnica_VF/blob/main/imagenes/matriz.png)

No se observa un fuerte correlaci√≥n entre las variables

2. **OTTO**

Este conjunto de datos esta en formato jsonl, es bastante pesado, para iniciar con su an√°lisis se cargo en el ambiente por lotes con un chun_size=10000, esta dividido en train y test en conjunto aprozimadamente 230M de registros con 5 columnas. 
**-Duplicados y valores faltantes**
No se observan valores faltantes, sin embargo hay varios duplicados, los cuales se proceden a borrarse
se analiza la distribuci√≥n de eventos tal como se muestra a continuaci√≥n:

## üîç Hallazgos Clave

Visualmente se observa el an√°lisis temporal y los articulos mas populares
<table>
  <tr>
    <td align="center">
      <img src="https://github.com/NestorSaenz/prueba_tecnica_VF/raw/main/imagenes/distribucion_eventos.jpg" width="95%" alt="Distribuci√≥n de Eventos">
      <br><em>Figura 1: Distribuci√≥n general de eventos</em>
    </td>
    <td align="center">
      <img src="https://github.com/NestorSaenz/prueba_tecnica_VF/raw/main/imagenes/grafica_distribucion_eventos.png" width="95%" alt="Gr√°fica de Distribuci√≥n">
      <br><em>Figura 2: Detalle de distribuci√≥n por categor√≠a</em>
    </td>
  </tr>
</table>



La mayoria de los eventos son interacciones a trav√©s de clics.

## üìà Din√°mica de Interacciones

<table>
  <tr>
    <td align="center">
      <img src="https://github.com/NestorSaenz/prueba_tecnica_VF/raw/main/imagenes/Analisis_temporal.png" width="100%" alt="An√°lisis Temporal">
      <br>
      <strong>Patrones Temporales</strong><br>
      <em>Figura 3</em>: Distribuci√≥n horaria/semanal de eventos<br>
      <small></small>
    </td>
    <td align="center">
      <img src="https://github.com/NestorSaenz/prueba_tecnica_VF/raw/main/imagenes/articulos_mas_interactuados.png" width="100%" alt="Top Art√≠culos">
      <br>
      <strong>Top Contenidos</strong><br>
      <em>Figura 4</em>: Art√≠culos con mayor interacci√≥n<br>
      <small></small>
    </td>
  </tr>
</table>

# ü§ñ Modelado
## 1. **Segmentaci√≥n de usuarios**
una vez entendido los datos se realiza lo que es la segmentacion de los clientes, para ello el set de datos se reduce a 50 componentes principales aplicando PCA

üéØ Objetivos
- Calcular puntuaciones de engagement de usuarios
- Segmentar clientes seg√∫n su comportamiento
- Identificar patrones clave en interacciones
- Crear segmentos de clientes accionables
  
 ### An√°lisis de Componentes Principales

#### PCA aplicado para reducir dimensionalidad
Ponderaci√≥n de componentes:
- PC_0: 50% (M√°s importante)
- PC_1: 30%
- PC_2: 15%
- PC_3: 5%

#### Caracteristicas analizadas
- Interacciones de usuarios
- Frecuencia de sesiones
- Patrones de actividad
- M√©tricas de uso

Para hacer la segmentaci√≥n se utiliz√≥ Los vecinos mas cercanos KMEANS, obteniendo los siguientes resultados:

<table>
  <tr>
    <td align="center">
      <img src="https://github.com/NestorSaenz/prueba_tecnica_VF/blob/main/imagenes/segmentacion.jpg" width="100%" alt="Resultados Segmentaci√≥n">
      <br>
      <strong>Patrones Temporales</strong><br>
      <em>Figura 3</em>: Resultados Segmentaci√≥n<br>
      <small></small>
    </td>
    <td align="center">
      <img src="https://github.com/NestorSaenz/prueba_tecnica_VF/blob/main/imagenes/grafico_segmentacion.png" width="100%" alt="Gr√°fico de la Segmentaci√≥n">
      <br>
      <strong>Top Contenidos</strong><br>
      <em>Figura 4</em>: Gr√°fico de la Segmentaci√≥n<br>
      <small></small>
    </td>
  </tr>
</table>
con la siguiente m√©trica de evaluaci√≥n *AUC-ROC: 0.7108018085367853*

En el siguiente grafico se observa la distribuci√≥n de la segmentaci√≥n

![Distribuci√≥n de la Segmentaci√≥n](https://github.com/NestorSaenz/prueba_tecnica_VF/blob/main/imagenes/grafica_segmentacion.png)


